{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c868192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /Users/veronicalobkina/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages (2.0.0)\n",
      "Collecting tweet-preprocessor\n",
      "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: tweet-preprocessor\n",
      "Successfully installed tweet-preprocessor-0.6.0\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7 MB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/veronicalobkina/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/veronicalobkina/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages (from transformers) (2021.8.3)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 55.9 MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/veronicalobkina/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages (from transformers) (21.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp37-cp37m-macosx_10_11_x86_64.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 43.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/veronicalobkina/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: importlib-metadata in /Users/veronicalobkina/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages (from transformers) (4.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/veronicalobkina/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/veronicalobkina/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: filelock in /Users/veronicalobkina/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/veronicalobkina/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/veronicalobkina/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/veronicalobkina/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/veronicalobkina/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/veronicalobkina/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/veronicalobkina/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/veronicalobkina/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages (from requests->transformers) (3.2)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.8.1 tokenizers-0.12.1 transformers-4.21.1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):\nmodule 'tensorflow_core.keras.activations' has no attribute 'swish'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonData/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonData/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonData/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonData/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonData/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonData/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonData/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages/transformers/models/bert/modeling_tf_bert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mactivations_tf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tf_activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from ...modeling_tf_outputs import (\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages/transformers/activations_tf.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0;34m\"silu\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswish\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0;34m\"swish\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswish\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_core.keras.activations' has no attribute 'swish'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/87/8wy1hy5d6wz1ydj6zlznqysc0000gn/T/ipykernel_42398/2102576492.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install transformers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFBertForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputExample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputFeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonData/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module {self.__name__} has no attribute {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    990\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             ) from e\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):\nmodule 'tensorflow_core.keras.activations' has no attribute 'swish'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "import json\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "!pip install emoji --upgrade\n",
    "import emoji\n",
    "\n",
    "!pip install tweet-preprocessor\n",
    "import preprocessor as p\n",
    "\n",
    "!pip install transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ac08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = []\n",
    "for dirname, _, filenames in os.walk('/Users/olgapodolska/Desktop/Ukraine_War/input/'):\n",
    "    for filename in filenames:\n",
    "        full_path=os.path.join(dirname, filename)\n",
    "        all_files.append(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60970883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the files\n",
    "all_files.sort()\n",
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886692ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch all August files - filenames containing \"AUG\" or \"202208\"\n",
    "aug_files = [file for file in all_files if re.search(\"/08\", file)]\n",
    "aug_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c0f097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the files and concatenate them into one pandas DataFrame\n",
    "tmp_df_list = []\n",
    "for file in aug_files:\n",
    "    print(f\"Reading in {file}\")\n",
    "    # unzip and read in the csv file as a dataframe\n",
    "    tmp_df = pd.read_csv(file, compression=\"gzip\", header=0, index_col=0)\n",
    "    # append dataframe to temp list\n",
    "    tmp_df_list.append(tmp_df)\n",
    "\n",
    "print(\"Concatenating the DataFrames\")\n",
    "# concatenate the dataframes in the temp list row-wise\n",
    "aug_df= pd.concat(tmp_df_list, axis=0)\n",
    "print(\"Concatenation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8772a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first 5 rows of the august dataframe\n",
    "aug_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f96f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get shape of the DataFrame\n",
    "print(f\"{aug_df.shape[0]} rows and {aug_df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee348ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "aug_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beff581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dtypes of usercreatedts, tweetcreatedts, and extractedts to datetime64 for easier operation later\n",
    "aug_df[\"usercreatedts\"] = pd.to_datetime(aug_df[\"usercreatedts\"])\n",
    "aug_df[\"tweetcreatedts\"] = pd.to_datetime(aug_df[\"tweetcreatedts\"])\n",
    "aug_df[\"extractedts\"] = pd.to_datetime(aug_df[\"extractedts\"])\n",
    "\n",
    "# check dtypes\n",
    "aug_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6e1658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When were the earliest and latest tweets in this dataset created\n",
    "earliest_tweet = aug_df[\"tweetcreatedts\"].min()\n",
    "latest_tweet = aug_df[\"tweetcreatedts\"].max()\n",
    "\n",
    "print(f\"The earliest tweet was at {earliest_tweet}, and the latest was at {latest_tweet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3749f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tweet frequency by date\n",
    "# get dates in the dataframe \n",
    "dates = aug_df[\"tweetcreatedts\"].dt.day\n",
    "# group tweet timestamps by date and get tweet count for each date\n",
    "tweetcount_by_date = aug_df[\"tweetcreatedts\"].groupby(dates).size()\n",
    "\n",
    "# plot bar graph of tweet count by date\n",
    "tweetcount_by_date.plot.bar();\n",
    "\n",
    "plt.title(\"August Tweet Count by Date\")\n",
    "plt.xlabel(\"Tweet Date\")\n",
    "plt.ylabel(\"Tweet Count\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d392b96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many languages are in this dataset\n",
    "print(f\"There are {aug_df['language'].nunique()} unique languages in this DataFrame.\")\n",
    "aug_df[\"language\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64900d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What percentage of the tweets is in English (en)\n",
    "print(f\"{round(aug_df.loc[aug_df['language']=='en'].shape[0]/aug_df.shape[0]*100, 2)}% of the tweets are in English.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cfdad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of different languages\n",
    "language_counts = aug_df.groupby(\"language\").size().sort_values(ascending=False)[0:20].plot.bar(figsize=(12,6),\n",
    "                                                                                         title=\"Top 20 Languages by Frequency\",\n",
    "                                                                                         xlabel=\"Language Code\",\n",
    "                                                                                         ylabel=\"Number of Tweets\",\n",
    "                                                                                         rot=90\n",
    "                                                                                         );\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c5bc6d",
   "metadata": {},
   "source": [
    "We can see that English (en) was by far the most prevalent language in this dataset, nearing 1.2 million tweets out of 1.96 million. The second and third most prevalent languages were French and Thai, respectively.\n",
    "\n",
    "Note that the forth most prevalent language was \"und\", which is used to indicate that Twitter could not detect a language. We can safely inspect English language only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef583a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect shortest and longest tweets\n",
    "min_len = aug_df[\"text\"].str.len().min()\n",
    "max_len = aug_df[\"text\"].str.len().max()\n",
    "\n",
    "\n",
    "print(f\"Shortest tweet has {min_len} chars.\")\n",
    "print(f\"Longest tweet has {max_len} chars.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efac282",
   "metadata": {},
   "source": [
    "Hold on, a tweet can have 280 characters max. How could one have more than the limit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec09562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get index of the tweet that has the max length\n",
    "max_len_index = aug_df[\"text\"].str.len().idxmax()\n",
    "# pull out the text of that index\n",
    "aug_df.loc[max_len_index, \"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cf055b",
   "metadata": {},
   "source": [
    "Upon research, mentions supposedly do not count toward the character limit when the tweet is a reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdc2910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of tweet lengths\n",
    "tweet_len_series = aug_df[\"text\"].str.len()\n",
    "tweet_len_series.plot.hist();\n",
    "plt.title(\"Distribution of Tweet Length\")\n",
    "plt.xlabel(\"Tweet Length (Characters)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "# draw a vertical line for the mean\n",
    "plt.axvline(x=tweet_len_series.mean(), color=\"red\")\n",
    "# draw a vertical line for the median\n",
    "plt.axvline(x=tweet_len_series.median(), color=\"yellow\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean: {tweet_len_series.mean()} chars\")\n",
    "print(f\"Median: {tweet_len_series.median()} chars\")\n",
    "print(f\"Standard deviation: {tweet_len_series.std()} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db361f71",
   "metadata": {},
   "source": [
    "The distribution is right-skewed. Most tweets appear to be below 300 characers in length. But because we have a few outlying tweets that have anomalously long lengths, as investigated above, the histogram has an elongated x-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d3434",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5024b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the non-English tweets\n",
    "eng_df=aug_df.loc[aug_df['language']=='en']\n",
    "eng_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cc4177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the tweets longer then 280 symbols\n",
    "short_df = eng_df.loc[eng_df[\"text\"].str.len() < 280]\n",
    "\n",
    "short_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7bc675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which columns have missing values\n",
    "short_df.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76ca2ee",
   "metadata": {},
   "source": [
    "Have missing values following columns: \n",
    "* acctdesc (account description), \n",
    "* location, \n",
    "* coordinates, \n",
    "* original_tweet_username, \n",
    "* in_reply_to_screen_name,\n",
    "* quoted_status_username "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dce464",
   "metadata": {},
   "source": [
    "acctdesc, original_tweet_username, in_reply_to_screen_name, quoted_status_username  columns contain information, we are not concerned at with this moment. For now, we will rely on the tweets to learn more about what kind of words are frequently used and the users' sentiments surrounding the war in Ukraine. Therefore, we will drop acctdesc column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b3601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the not important columns\n",
    "short_df.drop(\"acctdesc\", axis=1, inplace=True)\n",
    "# short_df.drop(\"original_tweet_username\", axis=1, inplace=True)\n",
    "# short_df.drop(\"in_reply_to_screen_name\", axis=1, inplace=True)\n",
    "# short_df.drop(\"quoted_status_username\", axis=1, inplace=True)\n",
    "# confirm it has been dropped\n",
    "short_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8574c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aswrangler as wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fafbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_s3_bucket = 'aiscience22'\n",
    "raw_path_dir = 'aug_Ukraine_War/'\n",
    "\n",
    "raw_path = f\"s3://{raw_s3_bucket}/{raw_path_dir}\n",
    "\n",
    "aug_df = wr.s3.read_csv(path=raw_path)\n",
    "\n",
    "aug_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4decbdf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
